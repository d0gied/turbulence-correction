# Turbulence Correction

Коррекция видео-турбулентности с использованием глубокого обучения. Проект позволяет удалять турбулентные искажения из видеопотоков с помощью обученной нейронной сети.

## Описание

Этот проект реализует систему для коррекции турбулентных искажений в видео. Включает:

- **Симулятор турбулентности** - генерирует синтетические видео с турбулентными искажениями для обучения
- **Модель восстановления** - нейронная сеть (на основе 3D сверток), обученная восстанавливать чистые видео из зашумленных
- **Инструменты инференса** - скрипты для применения обученной модели к видео

## Структура проекта

```
turbulence-correction/
├── simulation/              # Модуль симуляции турбулентности
│   └── turbulence/
│       ├── cli.py          # Командная строка для применения симулятора
│       ├── config.py       # Конфигурация параметров турбулентности
│       ├── simulator.py    # CPU версия симулятора
│       ├── simulator_gpu.py # GPU версия симулятора (Torch)
│       └── blur.py         # Вспомогательные функции размытия
├── notebooks/              # Jupyter ноутбуки
│       ├── train.ipynb     # Обучение модели
│       ├── inference.ipynb # Инференс с помощью модели
│       └── runs/           # Сохраненные checkpoints и логи обучения
├── data/                   # Набор данных
│   ├── images/
│   │   ├── clean/         # Чистые изображения
│   │   ├── synthetic_turbulence/  # Синтетически зашумленные
│   │   ├── synthetic_restored/    # Восстановленные из синтетических
│   │   ├── real_turbulence/       # Реальные зашумленные видео-кадры
│   │   └── real_restored/         # Восстановленные из реальных
│   └── videos/             # Аналогично для видео
├── inference.py            # Основной скрипт инференса
├── pyproject.toml          # Конфигурация проекта и зависимости
└── README.md               # Этот файл
```

## Требования

- Python >= 3.13
- PyTorch >= 2.10.0
- CUDA (опционально, для GPU ускорения)

## Использование

### Генерация синтетических данных с турбулентностью

```bash
python -m simulation.turbulence.cli --input-video <path_to_input> --output-video <path_to_output>
```

## Симуляция турбулентности

### Основные компоненты симулятора

#### 1. **Поле смещений (Displacement Field)**

Турбулентность моделируется как пространственное смещение пиксельной сетки. Смещение генерируется из **трехмерного шума Перлина**

**Параметры**:
- `turbulence_strength` - амплитуда смещения
- `turb_scale` - пространственный масштаб волн турбулентности
- `temporal_scale` - скорость временных изменений
- `levels` - количество октав в Перлин-шуме

#### 2. **Геометрическая деформация (Warping)**

#### 3. **Размытие (Gaussian Blur)**

**Параметры**:
- `sigma` - максимальное стандартное отклонение размытия
- `blur_levels` - количество предварительных уровней размытия


### Параметры конфигурации

```python
@dataclass
class TurbTorchConfig:
    turbulence_strength_px: float = 40.0      # Амплитуда (пиксели)
    temporal_smooth: float = 0.15             # Скорость изменения
    lowres_scale: int = 4                     # Шум на 1/4 разрешения
    noise_sigma_px_lowres: float = 3.0        # Пространственное сглаживание
    max_blur_sigma_px: float = 40.0
    blur_kernel_size: int = 31                # Размер ядра
    scintillation_beta: float = 0.15          # Мерцание интенсивности
    device: str = "cuda"                      # cuda или cpu
    dtype: torch.dtype = torch.float16        # float16 или float32
```

### Обучение модели

Используйте ноутбук [train.ipynb](notebooks/train.ipynb) для полного процесса обучения.

### Инференс - восстановление видео

```python
python inference.py --input <turbulent_video> --output <restored_video> --checkpoint <model_checkpoint>
```

## Архитектура модели

### Основная идея

Модель использует **видеоконтекст** для восстановления чистых кадров из турбулентных видеопотоков. Ключевая интуиция состоит в том, что информация о чистом содержимом сцены распределена по соседним кадрам видео. Турбулентное искажение разнится от кадра к кадру, поэтому усреднение информации из нескольких соседних кадров помогает избавиться от турбулентности, сохраняя при этом детали исходной сцены.

Архитектура построена следующим образом:

```
Входные кадры (T=5 кадров)
         ↓
    Stem (3 → 64 каналов)
         ↓
    10 × ResBlock3D (3D свертки + остатки)
         ↓
    TemporalWeightedPool (объединение по времени)
         ↓
    Head (декодирование в 3 канала RGB)
         ↓
    Выходной кадр + остаток исходного
```

### Компоненты модели

#### 1. **TurbRestore3DPlain** - Основная архитектура

- **Вход**: B,T,C,H,W (пакет размера B, T=5 кадров, C=3 каналов RGB, H×W пиксели)
- **Выход**: B,3,H,W (восстановленный кадр для каждого видео в пакете)

Особенности:
- Работает на **фиксированном разрешении** (без энкодера/декодера с понижением разрешения)
- Использует **остаточное соединение** (residual connection) от центрального кадра
- Применяет **групповую нормализацию** (GroupNorm) вместо BatchNorm для стабильности

```python
def forward(self, x):
    x = x.permute(0, 2, 1, 3, 4)  # B,T,C,H,W → B,C,T,H,W
    center = x[:, :, T//2]         # Центральный кадр (исходный)
    
    f = stem(x)                     # Извлечение признаков
    f = body(f)                     # Обработка через блоки
    f2d = pool(f)                   # Объединение по времени
    
    out = head(f2d)                 # Декодирование в RGB
    return (out + center).clamp(0, 1)  # Добавление остатка
```

#### 2. **ResBlock3D** - Остаточные 3D блоки

Трехмерные остаточные блоки для захвата как пространственных, так и временных особенностей:

```python
class ResBlock3D(nn.Module):
    def __init__(self, ch: int):
        self.n1 = nn.GroupNorm(8, ch)         # Нормализация
        self.c1 = conv3d(ch, ch, kernel=3)    # 3D свертка
        self.n2 = nn.GroupNorm(8, ch)
        self.c2 = conv3d(ch, ch, kernel=3)
    
    def forward(self, x):
        h = c1(silu(n1(x)))
        h = c2(silu(n2(h)))
        return x + h  # Остаточное соединение
```

**Стратегия**: Pre-activation (нормализация и активация перед сверткой)

#### 3. **TemporalWeightedPool** - Обучаемое объединение по времени

Вместо простого усреднения или максимума по оси времени, модель **обучает веса** для каждого кадра в окне:

```python
class TemporalWeightedPool(nn.Module):
    def forward(self, x):  # B,C,T,H,W
        a = self.logits(x)      # B,1,T,H,W - логиты для каждого кадра
        w = softmax(a, dim=T)   # B,1,T,H,W - нормированные веса
        y = (x * w).sum(dim=T)  # B,C,H,W   - взвешенная сумма
        return y
```

Модель самостоятельно решает, какие кадры более надежны для восстановления каждого пикселя. Кадры с сильной турбулентностью получат меньший вес, а кадры с хорошим качеством - больший.

### Параметры модели

- **base_ch** (по умолчанию 64) - количество базовых каналов
- **blocks** (по умолчанию 10) - количество ResBlock3D слоев
- **window** (по умолчанию 5) - размер временного окна

### Обработка видео

#### Скользящее окно

Видео обрабатывается с помощью **скользящего окна размером T кадров**:

```
Видео: [F0][F1][F2][F3][F4][F5][F6][F7]...
        
        [F0][F1][F2][F3][F4] → Восстановить F2
           [F1][F2][F3][F4][F5] → Восстановить F3
              [F2][F3][F4][F5][F6] → Восстановить F4
```

**Края видео**: Используется **репликация** первого и последнего кадра для обработки начала и конца видео.

#### Тайловая инференция (Tiling Inference)

Для обработки видео высокого разрешения модель разбивает кадр на перекрывающиеся плитки (tiles):

- **Размер плитки** (по умолчанию 256x256) - обрабатывается небольшими областями
- **Перекрытие** (по умолчанию 64 пиксела) - соседние плитки перекрываются для исключения артефактов на границах
- **Оконная функция Ханна** (Hann window) - применяется взвешивание для плавного слияния плиток

```python
def tile_coords(H, W, tile, overlap):
    # Вычисляет координаты углов плиток
    # Перекрытие 64px между соседними плитками обеспечивает гладкость
```

Позволяет обрабатывать видео любого разрешения.

### Числовые характеристики

- **Входные тензоры**: [B, T=5, C=3, H, W]
- **Выходные тензоры**: [B, 3, H, W]

### Функция потерь и обучение

Модель обучается на синтетических парах видео (турбулентное -> чистое), созданных симулятором. **Композитная функция потерь** состоит из двух компонент:

$$L = w_{L1} \cdot L_{L1} + w_{edge} \cdot L_{edge}$$

#### 1. **L1 Loss (Pixel-wise Reconstruction)**

$$L_{L1} = \frac{1}{N} \sum_{i} |y_{pred,i} - y_{gt,i}|$$

Основная компонента, обеспечивающая прямое пиксельное восстановление чистого кадра. **Вес по умолчанию**: $w_{L1} = 1.0$

#### 2. **Edge Loss (Gradient Matching)**

$$L_{edge} = \frac{1}{N} \sum_{i} |\nabla y_{pred,i} - \nabla y_{gt,i}|$$

где $\nabla$ - оператор Собеля для вычисления градиентов (краев) изображения.

Предотвращает размытие деталей и сохраняет резкость контуров объектов. **Вес по умолчанию**: $w_{edge} = 0.05$

**Назначение Edge Loss**: Турбулентность хорошо затирает мелкие детали; L1 loss один может привести к чрезмерному размытию восстановленного кадра. Edge loss принуждает модель сохранять четкие переходы интенсивности.
